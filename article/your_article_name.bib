Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@techreport{Jaeger2001,
author = {Jaeger, Herbert},
institution = {German National Research Center for Information Technology},
pages = {GMD Report 148},
title = {{The "echo state" approach to analysing and training recurrent neural networks}},
url = {http://www.faculty.jacobs-university.de/hjaeger/pubs/EchoStatesTechRep.pdf},
year = {2001}
}
@incollection{Lowe2011,
author = {Lowe, Robert and Mannella, Francesco and Ziemke, Tom and Baldassarre, Gianluca},
doi = {10.1007/978-3-642-21283-3_51},
file = {:home/vitay/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe et al. - 2011 - Modelling Coordination of Learning Systems A Reservoir Systems Approach to Dopamine Modulated Pavlovian Conditionin.pdf:pdf},
pages = {410--417},
publisher = {Springer Berlin Heidelberg},
title = {{Modelling Coordination of Learning Systems: A Reservoir Systems Approach to Dopamine Modulated Pavlovian Conditioning}},
url = {http://link.springer.com/10.1007/978-3-642-21283-3{\_}51},
year = {2011}
}
@book{Haykin2002,
abstract = {3rd ed. Ch. 1. Discrete-Time Signal Processing -- Ch. 2. Stationary Processes and Models -- Ch. 3. Spectrum Analyis -- Ch. 4. Eigenanalysis -- Ch. 5. Wiener Filters -- Ch. 6. Linear Prediction -- Ch. 7. Kalman Filters -- Ch. 8. Method of Steepest Descent -- Ch. 9. Least-Mean-Square Algorithm -- Ch. 10. Frequency-Domain Adaptive Filters -- Ch. 11. Method of Least Squares -- Ch. 12. Rotations and Reflections -- Ch. 13. Recursive Least-Squares Algorithm -- Ch. 14. Square-Root Adaptive Filters -- Ch. 15. Order-Recursive Adaptive Filters -- Ch. 16. Tracking of Time-Varying Systems -- Ch. 17. Fine-Precision Effects -- Ch. 18. Blind Deconvolution -- Ch. 19. Back-Propagation Learning -- Ch. 20. Radial Basis Funuction Networks -- Appendix A Complex Variables -- Appendix B Differentiation with Respect to a Vector -- Appendix C Method of Lagrange Multipliers -- Appendix D Estimation Theory -- Appendix E Maximum-Entropy Method.},
author = {Haykin, Simon S.},
isbn = {013322760X},
pages = {989},
publisher = {Prentice Hall},
title = {{Adaptive filter theory}},
year = {2002}
}
@article{Sussilo2009,
abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
author = {Sussillo, David and Abbott, L F},
doi = {10.1016/j.neuron.2009.07.018},
issn = {1097-4199},
journal = {Neuron},
month = {aug},
number = {4},
pages = {544--57},
pmid = {19709635},
title = {{Generating coherent patterns of activity from chaotic neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19709635},
volume = {63},
year = {2009}
}
@article{Sompolinsky1988,
author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
doi = {10.1103/PhysRevLett.61.259},
issn = {0031-9007},
journal = {Phys. Rev. Lett.},
month = {jul},
number = {3},
pages = {259--262},
publisher = {American Physical Society},
title = {{Chaos in Random Neural Networks}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.61.259},
volume = {61},
year = {1988}
}
@article{Laje2013,
abstract = {The brain's ability to tell time and produce complex spatiotemporal motor patterns is critical for anticipating the next ring of a telephone or playing a musical instrument. One class of models proposes that these abilities emerge from dynamically changing patterns of neural activity generated in recurrent neural networks. However, the relevant dynamic regimes of recurrent networks are highly sensitive to noise; that is, chaotic. We developed a firing rate model that tells time on the order of seconds and generates complex spatiotemporal patterns in the presence of high levels of noise. This is achieved through the tuning of the recurrent connections. The network operates in a dynamic regime that exhibits coexisting chaotic and locally stable trajectories. These stable patterns function as 'dynamic attractors' and provide a feature that is characteristic of biological systems: the ability to 'return' to the pattern being generated in the face of perturbations.},
author = {Laje, Rodrigo and Buonomano, Dean V},
doi = {10.1038/nn.3405},
issn = {1546-1726},
journal = {Nat. Neurosci.},
month = {jul},
number = {7},
pages = {925--33},
pmid = {23708144},
title = {{Robust timing and motor patterns by taming chaos in recurrent neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23708144},
volume = {16},
year = {2013}
}
@article{Maass2002,
abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
doi = {10.1162/089976602760407955},
issn = {0899-7667},
journal = {Neural Comput.},
month = {nov},
number = {11},
pages = {2531--60},
pmid = {12433288},
title = {{Real-time computing without stable states: a new framework for neural computation based on perturbations.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12433288},
volume = {14},
year = {2002}
}
