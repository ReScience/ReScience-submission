\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@sortscheme{nty}
\abx@aux@sortnamekeyscheme{global}
\abx@aux@cite{sutton1998introduction}
\abx@aux@cite{o2004dissociable}
\abx@aux@cite{behrens2007learning}
\abx@aux@cite{frank2004carrot}
\abx@aux@cite{frank2007genetic}
\abx@aux@cite{sharot2011unrealistic}
\abx@aux@cite{caze2013adaptive}
\abx@aux@cite{gershman2015learning}
\abx@aux@cite{garrett2014robust}
\abx@aux@cite{moutsiana2015human}
\abx@aux@cite{shah2016pessimistic}
\abx@aux@cite{garrett2017optimistic}
\abx@aux@cite{lefebvre2017behavioural}
\abx@aux@cite{palminteri2017confirmation}
\HyPL@Entry{0<</S/D>>}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{1}{section.2}}
\newlabel{methods}{{2}{1}{Methods}{section.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{2}{section.3}}
\newlabel{results}{{3}{2}{Results}{section.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{2}{section.4}}
\newlabel{conclusion}{{4}{2}{Conclusion}{section.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Average estimated Q-values after 800 trials averaged for different ratios of \(\alpha ^+\) and \(\alpha ^-\). The dotted lines represent the underlying average reward: 0.8, 0.6, -0.6, -0.8. The error bars represent the variance of the estimated Q-values.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:figure1}{{1}{3}{Average estimated Q-values after 800 trials averaged for different ratios of \(\alpha ^+\) and \(\alpha ^-\). The dotted lines represent the underlying average reward: 0.8, 0.6, -0.6, -0.8. The error bars represent the variance of the estimated Q-values.\relax }{figure.caption.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {A.} Performance, i.e.\nobreakspace {}proportion of choices for the best action, for the three agents: Rational (R, \(\alpha ^+=\alpha ^-\), blue line), Optimistic (O, \(\alpha ^+>\alpha ^-\), green line) and Pessimistic (P, \(\alpha ^+<\alpha ^-\), red line). In this figure and the following ones, the left (resp. right) panel corresponds to the low-reward (resp. high-reward) task. \textbf  {B.} Proportion of action switch after 800 trials for each agent, in the two different tasks.\relax }}{3}{figure.caption.2}}
\newlabel{fig:figure2}{{2}{3}{\textbf {A.} Performance, i.e.~proportion of choices for the best action, for the three agents: Rational (R, \(\alpha ^+=\alpha ^-\), blue line), Optimistic (O, \(\alpha ^+>\alpha ^-\), green line) and Pessimistic (P, \(\alpha ^+<\alpha ^-\), red line). In this figure and the following ones, the left (resp. right) panel corresponds to the low-reward (resp. high-reward) task. \textbf {B.} Proportion of action switch after 800 trials for each agent, in the two different tasks.\relax }{figure.caption.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The performances of the Meta-learner (N) are shown in \emph  {purple} and those of the Rational agents (R) in different colors of blue (in \emph  {teal} for \(\alpha = 0.01\), in \emph  {royal blue} for \(\alpha = 0.1\) and in \emph  {navy blue} for \(\alpha = 0.4\)).\relax }}{4}{figure.caption.3}}
\newlabel{fig:figure3}{{3}{4}{The performances of the Meta-learner (N) are shown in \emph {purple} and those of the Rational agents (R) in different colors of blue (in \emph {teal} for \(\alpha = 0.01\), in \emph {royal blue} for \(\alpha = 0.1\) and in \emph {navy blue} for \(\alpha = 0.4\)).\relax }{figure.caption.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The performances of the Meta-learner, Optimistic, Rational and Pessimistic agents \textbf  {A.} in a task where the probabilities of reward are 0.75 and 0.25 for the two choices. \textbf  {B.} in a ``three-armed bandit'' task.\relax }}{4}{figure.caption.4}}
\newlabel{fig:figure4}{{4}{4}{The performances of the Meta-learner, Optimistic, Rational and Pessimistic agents \textbf {A.} in a task where the probabilities of reward are 0.75 and 0.25 for the two choices. \textbf {B.} in a ``three-armed bandit'' task.\relax }{figure.caption.4}{}}
